{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujnAxFQ92bJ"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "RxJEDBuQOyuH",
        "outputId": "53ca87e6-b701-4c7f-d58d-691bf29fb73a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.dates as mpdate\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Link to the AWS data\n",
        "# https://drive.google.com/file/d/1zX7jL_mVJ8GNnNYhaQid_z8Cc4YoimVy/view?usp=sharing\n",
        "\n",
        "filename = \"data-cpu_utilization.csv\"\n",
        "\n",
        "data = pd.read_csv(filename)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DtRyI8JBE-o"
      },
      "source": [
        "EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "cc35wrcxBERk",
        "outputId": "f92654c6-0e85-4bc4-a05b-4a081e9fba61"
      },
      "outputs": [],
      "source": [
        "labels = data[\"Label\"].to_frame()\n",
        "labels[\"Count\"] = 0\n",
        "labels = labels.groupby([\"Label\"]).count().reset_index()\n",
        "\n",
        "plt.bar(labels[\"Label\"], height=labels[\"Count\"])\n",
        "plt.xticks([0, 1], [\"No Anomaly\", \"Anomaly\"])\n",
        "plt.ylabel(\"Count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "TBZ_qMHiB_wV",
        "outputId": "c49fa487-ab49-470d-eab0-90c45c4ef1b6"
      },
      "outputs": [],
      "source": [
        "anomaly_utils = data[data[\"Label\"] == 1]\n",
        "plt.hist(anomaly_utils[\"Utilization\"], bins=np.arange(10, 100, 5))\n",
        "plt.xlabel(\"CPU Utilization\")\n",
        "plt.ylabel(\"Anomalies\")\n",
        "plt.xticks(np.arange(10, 100, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9WyCmVy_2Tx"
      },
      "source": [
        "MLP Classifier using CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hMyT4vJ_4y4",
        "outputId": "8a1661bc-dd2f-41b8-f847-15713d8a8c74"
      },
      "outputs": [],
      "source": [
        "train_features, test_features, train_targets, test_targets  = train_test_split(data[[\"Utilization\"]], data[\"Label\"], test_size=0.2, train_size=0.8, shuffle=True, stratify=data[\"Label\"])\n",
        "hyper_param = {\"learning_rate_init\": [0.001, 0.005, 0.01, 0.05, 0.1],\n",
        "               \"hidden_layer_sizes\": [(100, ), (200, 50), (200, 100, 50), (200, 100, 50, 25), (200, 75, 50, 25, 5)]}\n",
        "norm_train = StandardScaler().fit_transform(train_features)\n",
        "gridSearchModel = MLPClassifier(max_iter=200)\n",
        "nested_cv = GridSearchCV(estimator=gridSearchModel, param_grid=hyper_param, scoring=\"f1\",cv=StratifiedKFold(n_splits=5, shuffle=True))\n",
        "nested_cv.fit(norm_train, train_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeEE2yIs_-AO"
      },
      "outputs": [],
      "source": [
        "gridSearchResults = pd.DataFrame(nested_cv.cv_results_)\n",
        "gridSearchResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmzZGDInzlHQ"
      },
      "outputs": [],
      "source": [
        "cleaned_results = gridSearchResults[[\"param_hidden_layer_sizes\", \"mean_test_score\"]]\n",
        "best_sizes = cleaned_results.groupby(\"param_hidden_layer_sizes\")[\"mean_test_score\"].max().reset_index().rename(columns={\"mean_test_score\" : \"Mean_F1_Score\", \"param_hidden_layer_sizes\": \"Hidden_Layer_Sizes\"})\n",
        "best_sizes.sort_values(by=\"Mean_F1_Score\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8216r8MhbjK"
      },
      "outputs": [],
      "source": [
        "nested_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkGKgFIuP9vL"
      },
      "outputs": [],
      "source": [
        "cross_val = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "normalizer = StandardScaler()\n",
        "accuracy = []\n",
        "loss = [] # using mse\n",
        "learning_rate = 0.01\n",
        "hidden_layers = (200, 100, 50, 25)\n",
        "\n",
        "for tdx, vdx in cross_val.split(train_features, train_targets):\n",
        "    fold_features, val_features = normalizer.fit_transform(train_features.iloc[tdx]), normalizer.fit_transform(train_features.iloc[vdx])\n",
        "    fold_targets, val_targets = train_targets.iloc[tdx], train_targets.iloc[vdx]\n",
        "    cv_mlp = MLPClassifier(hidden_layer_sizes=hidden_layers, learning_rate_init=learning_rate, max_iter=200).fit(fold_features, fold_targets)\n",
        "    accuracy.append(cv_mlp.score(X=val_features, y=val_targets))\n",
        "    loss.append(np.mean((cv_mlp.predict(val_features) - val_targets) ** 2))\n",
        "\n",
        "print(f\"Cross Validation Accuracy: {np.mean(accuracy)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2oNxmV7P_gO"
      },
      "outputs": [],
      "source": [
        "train_features, test_features, train_targets, test_targets  = train_test_split(data[[\"Utilization\"]], data[\"Label\"], test_size=0.2, train_size=0.8, shuffle=True, stratify=data[\"Label\"])\n",
        "mlp = MLPClassifier(hidden_layer_sizes=hidden_layers, learning_rate_init=learning_rate, max_iter=200).fit(normalizer.fit_transform(train_features), train_targets)\n",
        "mlp.score(normalizer.fit_transform(test_features), test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rFtBhbkAf2-"
      },
      "outputs": [],
      "source": [
        "plt.plot(mlp.loss_curve_)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.savefig(\"MLP Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA38dRFgRNkE"
      },
      "outputs": [],
      "source": [
        "def classify_prediction(row):\n",
        "    if row[\"Predicted\"] == 1 and row[\"Real\"] == 1:\n",
        "        return \"TP\"\n",
        "    elif row[\"Predicted\"] == 0 and row[\"Real\"] == 0:\n",
        "        return \"TN\"\n",
        "    elif row[\"Predicted\"] == 1 and row[\"Real\"] == 0:\n",
        "        return \"FP\"\n",
        "    else:\n",
        "        return \"FN\"\n",
        "predictions = mlp.predict(normalizer.fit_transform(test_features))\n",
        "real_v_predicted = pd.DataFrame({\"Predicted\": predictions, \"Real\": test_targets})\n",
        "real_v_predicted[\"Classification\"] = real_v_predicted.apply(classify_prediction, 1)\n",
        "TP = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"TP\"].count()\n",
        "TN = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"TN\"].count()\n",
        "FP = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"FP\"].count()\n",
        "FN = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"FN\"].count()\n",
        "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
        "\n",
        "recall = lambda tp, fn: tp / (tp + fn)\n",
        "precision = lambda tp, fp: tp / (tp + fp)\n",
        "f1 = lambda r, p: 2 * r * p / (p + r)\n",
        "\n",
        "print(f\"Recall: {recall(TP, FN)}, Precision: {precision(TP, FP)}, F1: {f1(recall(TP, FN), precision(TP, FP))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSoph-XLR-9R"
      },
      "outputs": [],
      "source": [
        "c_matrix = confusion_matrix(y_true=test_targets, y_pred=predictions, labels=[0, 1])\n",
        "display = ConfusionMatrixDisplay(c_matrix)\n",
        "display.plot()\n",
        "plt.savefig(\"mlpconfusion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn8nbXDpN41y"
      },
      "outputs": [],
      "source": [
        "def create_cm(predictions, targets, name):\n",
        "  c_matrix = confusion_matrix(y_true=targets, y_pred=predictions, labels=[0, 1])\n",
        "  display = ConfusionMatrixDisplay(c_matrix)\n",
        "  display.plot()\n",
        "  plt.savefig(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbD2ozJnJAEM"
      },
      "source": [
        "Single Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKLELkkGI_bz"
      },
      "outputs": [],
      "source": [
        "cross_val = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "normalizer = StandardScaler()\n",
        "train_features, test_features, train_targets, test_targets  = train_test_split(data[[\"Utilization\"]], data[\"Label\"], test_size=0.2, train_size=0.8, shuffle=True, stratify=data[\"Label\"])\n",
        "accuracy = []\n",
        "loss = [] # using mse\n",
        "\n",
        "for tdx, vdx in cross_val.split(train_features, train_targets):\n",
        "    fold_features, val_features = normalizer.fit_transform(train_features.iloc[tdx]), normalizer.fit_transform(train_features.iloc[vdx])\n",
        "    fold_targets, val_targets = train_targets.iloc[tdx], train_targets.iloc[vdx]\n",
        "    cv_slp = Perceptron(max_iter=1000, tol=1e-3).fit(fold_features, fold_targets)\n",
        "    accuracy.append(cv_slp.score(X=val_features, y=val_targets))\n",
        "    loss.append(np.mean((cv_slp.predict(val_features) - val_targets) ** 2))\n",
        "\n",
        "print(f\"Cross Validation Accuracy: {np.mean(accuracy)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tockekyzJLeY"
      },
      "outputs": [],
      "source": [
        "normalizer = StandardScaler()\n",
        "train_features, test_features, train_targets, test_targets  = train_test_split(data[[\"Utilization\"]], data[\"Label\"], test_size=0.2, train_size=0.8, shuffle=True, stratify=data[\"Label\"])\n",
        "slp = Perceptron(max_iter=1000, tol=1e-3).fit(normalizer.fit_transform(train_features), train_targets)\n",
        "print(slp.score(normalizer.fit_transform(test_features), test_targets))\n",
        "\n",
        "\n",
        "predictions = slp.predict(normalizer.fit_transform(test_features))\n",
        "real_v_predicted = pd.DataFrame({\"Predicted\": predictions, \"Real\": test_targets})\n",
        "real_v_predicted[\"Classification\"] = real_v_predicted.apply(classify_prediction, 1)\n",
        "TP = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"TP\"].count()\n",
        "TN = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"TN\"].count()\n",
        "FP = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"FP\"].count()\n",
        "FN = real_v_predicted[\"Classification\"][real_v_predicted[\"Classification\"] == \"FN\"].count()\n",
        "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
        "\n",
        "recall = lambda tp, fn: tp / (tp + fn)\n",
        "precision = lambda tp, fp: tp / (tp + fp)\n",
        "f1 = lambda r, p: 2 * r * p / (p + r)\n",
        "\n",
        "print(f\"Recall: {recall(TP, FN)}, Precision: {precision(TP, FP)}, F1: {f1(recall(TP, FN), precision(TP, FP))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "K93k64o4N2Rv",
        "outputId": "7fa0bd1a-776a-4fdd-d004-8dc2709eeaef"
      },
      "outputs": [],
      "source": [
        "create_cm(slp.predict(normalizer.fit_transform(test_features)), test_targets, \"slpconfusion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LE1sobo960Z"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFsyBQHOuO4X"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "class MyLogisticRegression:\n",
        "  def __init__(self, learning_rate:float, num_epoch:int, threshold:float=0.5):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_epoch = num_epoch\n",
        "    self.threshold = threshold\n",
        "    self.w = None\n",
        "    self.bias = None\n",
        "    self.misclassified_samples = []\n",
        "\n",
        "  def fit(self, x: pd.DataFrame, y: pd.DataFrame):\n",
        "    # Initialize weights to 1\n",
        "    numRows, numCols = x.shape\n",
        "    self.w = np.zeros((numCols, 1))\n",
        "    self.bias = 0\n",
        "    self.misclassified_samples = []\n",
        "\n",
        "    x = x.to_numpy()\n",
        "    y = np.array([y.to_numpy()]).T    # Make the array have dimensions 1 x n\n",
        "\n",
        "    for epoch in range(self.num_epoch):\n",
        "      # Shuffle dataset\n",
        "      x, y = shuffle(x, y)\n",
        "\n",
        "      linear_pred = np.dot(x, self.w) + self.bias\n",
        "      predictions = self.sigmoid(linear_pred)\n",
        "\n",
        "      # Update weights\n",
        "      w_gradient = (1 / numRows) * np.dot(x.T, (predictions - y))\n",
        "      self.w -= self.learning_rate * w_gradient\n",
        "\n",
        "      bias_gradient = (1 / numRows) * np.sum(predictions - y)\n",
        "      self.bias -= self.learning_rate * bias_gradient\n",
        "\n",
        "      # Record count of the errors during this training iteration\n",
        "      errors = np.array([[(1 if pred >= self.threshold else 0) for pred in predictions]]).T - y\n",
        "      self.misclassified_samples.append(np.count_nonzero(errors))\n",
        "\n",
        "  def sigmoid(self, z: float) -> int:\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  def predict(self, x: pd.DataFrame):\n",
        "    linear_pred = np.dot(x, self.w) + self.bias\n",
        "    y_predictions = self.sigmoid(linear_pred)\n",
        "    class_predictions = [(1 if y >= self.threshold else 0) for y in y_predictions]\n",
        "    return class_predictions\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU-aqoqj9_X5",
        "outputId": "e403062e-c3e6-4dff-d8af-98ba5672760d"
      },
      "outputs": [],
      "source": [
        "cross_val = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "normalizer = StandardScaler()\n",
        "train_features, test_features, train_targets, test_targets = \\\n",
        "    train_test_split(data[[\"Utilization\"]],\n",
        "                     data[\"Label\"],\n",
        "                     test_size=0.2,\n",
        "                     train_size=0.8,\n",
        "                     shuffle=True,\n",
        "                     stratify=data[\"Label\"])\n",
        "cv_accuracy = []\n",
        "mse = []\n",
        "\n",
        "for tdx, vdx in cross_val.split(train_features, train_targets):\n",
        "    fold_features = normalizer.fit_transform(train_features.iloc[tdx])\n",
        "    val_features = normalizer.fit_transform(train_features.iloc[vdx])\n",
        "    fold_targets = train_targets.iloc[tdx]\n",
        "    val_targets = train_targets.iloc[vdx]\n",
        "\n",
        "    cv_logReg = LogisticRegression(max_iter=1000, tol=1e-3).fit(fold_features, fold_targets)\n",
        "    cv_accuracy.append(cv_logReg.score(X=val_features, y=val_targets))\n",
        "    mse.append(np.mean((cv_logReg.predict(val_features) - val_targets) ** 2))\n",
        "\n",
        "print(f\"Cross Validation Accuracy: {np.mean(cv_accuracy)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpF00MMxMNdg"
      },
      "outputs": [],
      "source": [
        "normalizer = StandardScaler()\n",
        "train_features, test_features, train_targets, test_targets = \\\n",
        "    train_test_split(data[[\"Utilization\"]],\n",
        "                     data[\"Label\"],\n",
        "                     test_size=0.2,\n",
        "                     train_size=0.8,\n",
        "                     shuffle=True,\n",
        "                     stratify=data[\"Label\"])\n",
        "\n",
        "# logReg = LogisticRegression(tol=1e-3)\n",
        "# logReg.fit(train_features, train_targets)\n",
        "# print(\"Weights of\", logReg.coef_)\n",
        "# print(\"Bias of\", logReg.intercept_[0])\n",
        "# logReg.score(test_features, test_targets)\n",
        "\n",
        "logReg = MyLogisticRegression(learning_rate=0.01, num_epoch=1000)\n",
        "logReg.fit(train_features, train_targets)\n",
        "predictions = logReg.predict(test_features)\n",
        "print(\"My logistic regression accuracy:\", accuracy(test_targets, predictions))\n",
        "plt.plot(range(1, len(logReg.misclassified_samples) + 1),\n",
        "         logReg.misclassified_samples, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Errors')\n",
        "plt.title('Sum of Errors vs. Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "iP77OwLDPVup",
        "outputId": "275c3adb-e0e5-4802-ab1b-465f80a55abd"
      },
      "outputs": [],
      "source": [
        "create_cm(predictions, test_targets, \"lrconfusion\")\n",
        "plt.title(\"Logistic Regression Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DzzXajmlLxe"
      },
      "source": [
        "K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4ggW8zlqHhq"
      },
      "outputs": [],
      "source": [
        "#RUN ONCE\n",
        "# converts timestamp into unix time\n",
        "def to_float(time):\n",
        "     return time.timestamp()\n",
        "\n",
        "data[\"Time Stamp\"] = data[\"Time Stamp\"].apply(pd.Timestamp, 0).apply(mpdate.date2num, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hUbojTDolLg2",
        "outputId": "aad1943e-5b4e-430d-a0c4-a248f8f2f7fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "x = data[\"Time Stamp\"]\n",
        "y = data.Label\n",
        "z = data.Utilization\n",
        "\n",
        "x_normalized = MinMaxScaler().fit_transform(np.array(x).reshape(-1, 1))\n",
        "y_normalized = MinMaxScaler().fit_transform(np.array(y).reshape(-1, 1))\n",
        "z_normalized = MinMaxScaler().fit_transform(np.array(z).reshape(-1, 1))\n",
        "\n",
        "#time label\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_normalized, y_normalized, test_size=0.2, random_state=1234\n",
        ")\n",
        "\n",
        "#time util\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(\n",
        "    x_normalized, z_normalized, test_size=0.2, random_state=1234\n",
        ")\n",
        "\n",
        "\n",
        "#util label\n",
        "x3_train, x3_test, y3_train, y3_test = train_test_split(\n",
        "    z_normalized, y_normalized, test_size=0.2, random_state=1234\n",
        ")\n",
        "# splitting into normalized test/train\n",
        "\n",
        "\n",
        "\n",
        "# dataframes of X and Y respectively\n",
        "timeLabel = pd.DataFrame({'Time':x_train.flatten(),'Label':y_train.flatten()})\n",
        "timeUtil = pd.DataFrame({'Time':x2_train.flatten(),'Utilization':y2_train.flatten()})\n",
        "UtilLabel = pd.DataFrame({'Utilization':x3_train.flatten(),'Label':y3_train.flatten()})\n",
        "\n",
        "\n",
        "#graphics for slides\n",
        "\n",
        "#utilization and label visualized\n",
        "fig, ax = plt.subplots()\n",
        "sns.scatterplot(data = UtilLabel ,x=\"Utilization\",y=\"Label\")\n",
        "plt.show()\n",
        "\n",
        "#time and label visualized\n",
        "fig, ax = plt.subplots()\n",
        "sns.scatterplot(data = timeLabel ,x=\"Time\",y=\"Label\")\n",
        "plt.show()\n",
        "\n",
        "#time and utilization visualized\n",
        "fig, ax = plt.subplots()\n",
        "sns.scatterplot(data = timeUtil ,x=\"Time\",y=\"Utilization\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "Hbxn7qPFnxi_",
        "outputId": "927da2c2-b2c1-4b2c-c3c5-75d45aaa51ba"
      },
      "outputs": [],
      "source": [
        "elbow = []\n",
        "K = range(1,11)\n",
        "for i in range(1, 11):\n",
        "  kmeans = KMeans(n_clusters = i, random_state = 0, n_init='auto')\n",
        "  kmeans.fit(x2_train)\n",
        "  elbow.append(kmeans.inertia_)\n",
        "\n",
        "print(elbow)\n",
        "sns.lineplot(x = K, y = elbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "YFcvPFXLnxYE",
        "outputId": "313d4be7-5101-43ab-c042-1292cc26fe64"
      },
      "outputs": [],
      "source": [
        "#graphic for slides\n",
        "#util and label, clusters visualized\n",
        "kmeans = KMeans(n_clusters = 9, random_state = 0, n_init='auto')\n",
        "kmeans.fit(x3_train)\n",
        "sns.scatterplot(data = UtilLabel, x = 'Utilization', y = 'Label', hue = kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "3vk1tQcrrzFQ",
        "outputId": "65c02c5f-89d8-4418-fb08-e5d16b89e475"
      },
      "outputs": [],
      "source": [
        "\n",
        "#graphic for slides\n",
        "#time and label, clusters visualized\n",
        "kmeans = KMeans(n_clusters = 9, random_state = 0, n_init='auto')\n",
        "kmeans.fit(x2_train)\n",
        "sns.scatterplot(data = timeLabel, x = 'Time', y = 'Label', hue = kmeans.labels_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "bdIHF9Tjn6vQ",
        "outputId": "f344b26f-eebb-42d5-bf8e-9a49a16e10ba"
      },
      "outputs": [],
      "source": [
        "#important one that looks to be good numerically until you find out its really not\n",
        "\n",
        "# time X utilization, then assign values?/anoms?\n",
        "x = data[\"Time Stamp\"]\n",
        "y = data.Utilization\n",
        "z = data.Label\n",
        "\n",
        "x_normalized = MinMaxScaler().fit_transform(np.array(x).reshape(-1, 1))\n",
        "y_normalized = MinMaxScaler().fit_transform(np.array(y).reshape(-1, 1))\n",
        "z_normalized = MinMaxScaler().fit_transform(np.array(z).reshape(-1, 1))\n",
        "\n",
        "x2_train, x2_test, y2_train, y2_test, label_train, label_test = train_test_split(\n",
        "    x_normalized, y_normalized, z_normalized, test_size=0.2, random_state=1234\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "timeUtil = pd.DataFrame({'Time':x2_train.flatten(),'Utilization':y2_train.flatten()})\n",
        "\n",
        "\n",
        "\n",
        "#fit the model and display it by clusters\n",
        "kmeans = KMeans(n_clusters = 3, random_state = 0, n_init='auto')\n",
        "kmeans.fit(x2_train)\n",
        "#graphic in slides\n",
        "sns.scatterplot(data = timeUtil, x = 'Time', y = 'Utilization', hue = kmeans.labels_)\n",
        "\n",
        "\n",
        "\n",
        "lis = list(zip(x2_train.flatten(),label_train.flatten()))\n",
        "anoma = [0]*kmeans.cluster_centers_.size\n",
        "fine = [0]*kmeans.cluster_centers_.size\n",
        "#count how many regular points and anomalies are in each cluster\n",
        "for i in range(len(lis)):\n",
        "  if lis[i][1] > 0:\n",
        "    anoma[kmeans.labels_[i]] += 1\n",
        "  else:\n",
        "    fine[kmeans.labels_[i]] += 1\n",
        "#0th address in fine[] is how many regular points are in the 0th cluster\n",
        "#0th address in anomaly[] is how many anomalies are in the 0th cluster\n",
        "print(\"fine:\",fine)\n",
        "print(\"anomaly:\",anoma)\n",
        "anomPerc = [0]*kmeans.cluster_centers_.size\n",
        "\n",
        "# % of points in that cluster that are anomalies\n",
        "for x in range(len(anomPerc)):\n",
        "  anomPerc[x] = (anoma[x] / (anoma[x] + fine[x]))\n",
        "print(\"Anomaly % for each cluster:\",anomPerc)\n",
        "\n",
        "\n",
        "#now we can predict?\n",
        "predicted = kmeans.predict(x2_test)\n",
        "\n",
        "#homemade confusion matrix just to double check\n",
        "CM = [0]*4\n",
        "# TT, FF, TF, FT\n",
        "#[320, 3260, 30, 0]\n",
        "CMreal = [0] * len(y2_test)\n",
        "for b in range(len(predicted)):\n",
        "  pred = False\n",
        "  if anomPerc[predicted[b]] > .2:\n",
        "    pred = True\n",
        "    CMreal[b] = 1\n",
        "  if pred == True and label_test[b] == 1:\n",
        "    CM[0] += 1\n",
        "  if pred == False and label_test[b] == 0:\n",
        "    CM[1] += 1\n",
        "  if pred == True and label_test[b] == 0:\n",
        "    CM[2] += 1\n",
        "  if pred == False and label_test[b] == 1:\n",
        "    CM[3] += 1\n",
        "print(CM)\n",
        "\n",
        "\n",
        "#cm in slides\n",
        "c_matrix = confusion_matrix(y_true=label_test, y_pred=CMreal, labels=[0, 1])\n",
        "display = ConfusionMatrixDisplay(c_matrix)\n",
        "display.plot()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run through every point, if the point is an anomaly give its cluster +1, find the cluster with the most\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "KKMXccZHgAzr",
        "outputId": "7189ad47-1f3c-454e-f4db-229081d19493"
      },
      "outputs": [],
      "source": [
        "\n",
        "#visualization & CM for k = 9, which is what was originally used for the presentation\n",
        "\n",
        "\n",
        "#fit the model and display it by clusters\n",
        "kmeans = KMeans(n_clusters = 9, random_state = 0, n_init='auto')\n",
        "kmeans.fit(x2_train)\n",
        "#graphic in slides\n",
        "sns.scatterplot(data = timeUtil, x = 'Time', y = 'Utilization', hue = kmeans.labels_)\n",
        "\n",
        "\n",
        "\n",
        "lis = list(zip(x2_train.flatten(),label_train.flatten()))\n",
        "anoma = [0]*kmeans.cluster_centers_.size\n",
        "fine = [0]*kmeans.cluster_centers_.size\n",
        "#count how many regular points and anomalies are in each cluster\n",
        "for i in range(len(lis)):\n",
        "  if lis[i][1] > 0:\n",
        "    anoma[kmeans.labels_[i]] += 1\n",
        "  else:\n",
        "    fine[kmeans.labels_[i]] += 1\n",
        "#0th address in fine[] is how many regular points are in the 0th cluster\n",
        "#0th address in anomaly[] is how many anomalies are in the 0th cluster\n",
        "print(\"fine:\",fine)\n",
        "print(\"anomaly:\",anoma)\n",
        "anomPerc = [0]*kmeans.cluster_centers_.size\n",
        "\n",
        "# % of points in that cluster that are anomalies\n",
        "for x in range(len(anomPerc)):\n",
        "  anomPerc[x] = (anoma[x] / (anoma[x] + fine[x]))\n",
        "print(\"Anomaly % for each cluster:\",anomPerc)\n",
        "\n",
        "\n",
        "#now we can predict?\n",
        "predicted = kmeans.predict(x2_test)\n",
        "\n",
        "#homemade confusion matrix just to double check\n",
        "CM = [0]*4\n",
        "# TT, FF, TF, FT\n",
        "#[320, 3260, 30, 0]\n",
        "CMreal = [0] * len(y2_test)\n",
        "for b in range(len(predicted)):\n",
        "  pred = False\n",
        "  if anomPerc[predicted[b]] > .2:\n",
        "    pred = True\n",
        "    CMreal[b] = 1\n",
        "  if pred == True and label_test[b] == 1:\n",
        "    CM[0] += 1\n",
        "  if pred == False and label_test[b] == 0:\n",
        "    CM[1] += 1\n",
        "  if pred == True and label_test[b] == 0:\n",
        "    CM[2] += 1\n",
        "  if pred == False and label_test[b] == 1:\n",
        "    CM[3] += 1\n",
        "print(CM)\n",
        "\n",
        "\n",
        "#cm in slides\n",
        "c_matrix = confusion_matrix(y_true=label_test, y_pred=CMreal, labels=[0, 1])\n",
        "display = ConfusionMatrixDisplay(c_matrix)\n",
        "display.plot()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run through every point, if the point is an anomaly give its cluster +1, find the cluster with the most\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
